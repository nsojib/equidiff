{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No private macro file found!\n",
      "    It is recommended to use a private macro file\n",
      "    To setup, run: python /home/ubuntu/miniconda3/envs/equidiff/lib/python3.9/site-packages/robomimic/scripts/setup_macros.py\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "import sys\n",
    "import socket\n",
    "import traceback\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import mimicgen_environments\n",
    "import robomimic\n",
    "import robomimic.utils.train_utils as TrainUtils\n",
    "import robomimic.utils.torch_utils as TorchUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "from robomimic.config import config_factory\n",
    "from robomimic.algo import algo_factory, RolloutPolicy\n",
    "from robomimic.utils.log_utils import PrintLogger, DataLogger, flush_warnings\n",
    "\n",
    "import h5py\n",
    " \n",
    "# import mimicgen.utils.file_utils as MG_FileUtils\n",
    "# import mimicgen.utils.robomimic_utils as RobomimicUtils\n",
    " \n",
    "\n",
    "from dataset_mod_seg import MultiSegmentSequenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/home/ubuntu/bc_trans124.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_file:  /home/ubuntu/bc_trans124.json\n",
      "data file:  /home/ubuntu/dataset_mimicgen/square134_2_0ind_g40b30.hdf5\n",
      "device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "ext_cfg = json.load(open(config_file, 'r'))\n",
    "config = config_factory(ext_cfg[\"algo_name\"])\n",
    "# update config with external json - this will throw errors if\n",
    "# the external config has keys not present in the base algo config\n",
    "with config.values_unlocked():\n",
    "    config.update(ext_cfg)\n",
    "config.lock()\n",
    "\n",
    "device = TorchUtils.get_torch_device(try_to_use_cuda=config.train.cuda)\n",
    "\n",
    "print('config_file: ', config_file)\n",
    "print('data file: ', config.train.data) \n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']\n",
      "using obs modality: rgb with keys: ['agentview_image', 'robot0_eye_in_hand_image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n",
      "obs key agentview_image with shape (84, 84, 3)\n",
      "obs key robot0_eef_pos with shape (3,)\n",
      "obs key robot0_eef_quat with shape (4,)\n",
      "obs key robot0_eye_in_hand_image with shape (84, 84, 3)\n",
      "obs key robot0_gripper_qpos with shape (2,)\n"
     ]
    }
   ],
   "source": [
    "# first set seeds\n",
    "np.random.seed(config.train.seed)\n",
    "torch.manual_seed(config.train.seed)\n",
    "\n",
    "# torch.set_num_threads(2)\n",
    "\n",
    "# read config to set up metadata for observation modalities (e.g. detecting rgb observations)\n",
    "ObsUtils.initialize_obs_utils_with_config(config)\n",
    "\n",
    "# make sure the dataset exists\n",
    "dataset_path = os.path.expanduser(config.train.data)\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise Exception(\"Dataset at provided path {} not found!\".format(dataset_path))\n",
    " \n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path=config.train.data)\n",
    "shape_meta = FileUtils.get_shape_metadata_from_dataset(\n",
    "    dataset_path=config.train.data,\n",
    "    all_obs_keys=config.all_obs_keys,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path=config.train.data\n",
    "file=h5py.File(data_path, 'r')\n",
    "demos=file['data'].keys()\n",
    "# demos=[b.decode('utf-8') for b in file['mask'][config.train.hdf5_filter_key]]\n",
    "\n",
    "segs_orginal={}\n",
    "for demo in demos: \n",
    "    segs_orginal[demo]=[ [0, file['data'][demo].attrs['num_samples']-1]  ]\n",
    "\n",
    "file.close()\n",
    "len(segs_orginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'demo_0': [[0, 332]],\n",
       " 'demo_1': [[0, 370]],\n",
       " 'demo_10': [[0, 375]],\n",
       " 'demo_11': [[0, 383]],\n",
       " 'demo_12': [[0, 416]],\n",
       " 'demo_13': [[0, 413]],\n",
       " 'demo_14': [[0, 979]],\n",
       " 'demo_15': [[0, 673]],\n",
       " 'demo_16': [[0, 376]],\n",
       " 'demo_17': [[0, 617]],\n",
       " 'demo_18': [[0, 662]],\n",
       " 'demo_19': [[0, 1014]],\n",
       " 'demo_2': [[0, 730]],\n",
       " 'demo_20': [[0, 1120]],\n",
       " 'demo_21': [[0, 447]],\n",
       " 'demo_22': [[0, 393]],\n",
       " 'demo_23': [[0, 753]],\n",
       " 'demo_24': [[0, 968]],\n",
       " 'demo_25': [[0, 1134]],\n",
       " 'demo_26': [[0, 735]],\n",
       " 'demo_27': [[0, 594]],\n",
       " 'demo_28': [[0, 329]],\n",
       " 'demo_29': [[0, 388]],\n",
       " 'demo_3': [[0, 544]],\n",
       " 'demo_30': [[0, 451]],\n",
       " 'demo_31': [[0, 435]],\n",
       " 'demo_32': [[0, 357]],\n",
       " 'demo_33': [[0, 359]],\n",
       " 'demo_34': [[0, 451]],\n",
       " 'demo_35': [[0, 400]],\n",
       " 'demo_36': [[0, 374]],\n",
       " 'demo_37': [[0, 397]],\n",
       " 'demo_38': [[0, 347]],\n",
       " 'demo_39': [[0, 378]],\n",
       " 'demo_4': [[0, 412]],\n",
       " 'demo_40': [[0, 412]],\n",
       " 'demo_41': [[0, 373]],\n",
       " 'demo_42': [[0, 352]],\n",
       " 'demo_43': [[0, 395]],\n",
       " 'demo_44': [[0, 365]],\n",
       " 'demo_45': [[0, 384]],\n",
       " 'demo_46': [[0, 367]],\n",
       " 'demo_47': [[0, 332]],\n",
       " 'demo_48': [[0, 411]],\n",
       " 'demo_49': [[0, 380]],\n",
       " 'demo_5': [[0, 373]],\n",
       " 'demo_50': [[0, 354]],\n",
       " 'demo_51': [[0, 427]],\n",
       " 'demo_52': [[0, 408]],\n",
       " 'demo_53': [[0, 387]],\n",
       " 'demo_54': [[0, 392]],\n",
       " 'demo_55': [[0, 384]],\n",
       " 'demo_56': [[0, 354]],\n",
       " 'demo_57': [[0, 538]],\n",
       " 'demo_58': [[0, 790]],\n",
       " 'demo_59': [[0, 701]],\n",
       " 'demo_6': [[0, 488]],\n",
       " 'demo_60': [[0, 644]],\n",
       " 'demo_61': [[0, 590]],\n",
       " 'demo_62': [[0, 469]],\n",
       " 'demo_63': [[0, 715]],\n",
       " 'demo_64': [[0, 624]],\n",
       " 'demo_65': [[0, 544]],\n",
       " 'demo_66': [[0, 686]],\n",
       " 'demo_67': [[0, 684]],\n",
       " 'demo_68': [[0, 1083]],\n",
       " 'demo_69': [[0, 727]],\n",
       " 'demo_7': [[0, 780]],\n",
       " 'demo_8': [[0, 602]],\n",
       " 'demo_9': [[0, 993]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs_orginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subtasks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = config.train.data\n",
    "filter_key = config.train.hdf5_filter_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train.hdf5_filter_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_map={'demo_0': [[0, np.int64(332)]],\n",
    " 'demo_1': [(np.int64(0), np.int64(152))],\n",
    " 'demo_10': [[0, np.int64(488)]],\n",
    " 'demo_100': [[0, np.int64(538)]],\n",
    " 'demo_101': [[0, np.int64(790)]],\n",
    " 'demo_102': [[0, np.int64(701)]],\n",
    " 'demo_103': [[0, np.int64(644)]],\n",
    " 'demo_104': [[0, np.int64(590)]],\n",
    " 'demo_105': [[0, np.int64(469)]],\n",
    " 'demo_106': [[0, np.int64(715)]],\n",
    " 'demo_107': [[0, np.int64(624)]],\n",
    " 'demo_108': [[0, np.int64(544)]],\n",
    " 'demo_109': [[0, np.int64(686)]],\n",
    " 'demo_11': [[0, np.int64(780)]],\n",
    " 'demo_110': [[0, np.int64(684)]],\n",
    " 'demo_111': [[0, np.int64(1083)]],\n",
    " 'demo_112': [[0, np.int64(727)]],\n",
    " 'demo_113': [[0, np.int64(766)]],\n",
    " 'demo_114': [[0, np.int64(489)]],\n",
    " 'demo_115': [[0, np.int64(485)]],\n",
    " 'demo_116': [[0, np.int64(551)]],\n",
    " 'demo_117': [[0, np.int64(693)]],\n",
    " 'demo_118': [[0, np.int64(820)]],\n",
    " 'demo_119': [[0, np.int64(1031)]],\n",
    " 'demo_12': [[0, np.int64(602)]],\n",
    " 'demo_120': [[0, np.int64(627)]],\n",
    " 'demo_121': [[0, np.int64(495)]],\n",
    " 'demo_122': [[0, np.int64(572)]],\n",
    " 'demo_123': [[0, np.int64(585)]],\n",
    " 'demo_124': [[0, np.int64(575)]],\n",
    " 'demo_125': [[0, np.int64(610)]],\n",
    " 'demo_126': [[0, np.int64(1122)]],\n",
    " 'demo_127': [[0, np.int64(438)]],\n",
    " 'demo_128': [[0, np.int64(625)]],\n",
    " 'demo_129': [[0, np.int64(456)]],\n",
    " 'demo_13': [[0, np.int64(993)]],\n",
    " 'demo_14': [[0, np.int64(830)]],\n",
    " 'demo_15': [[0, np.int64(375)]],\n",
    " 'demo_16': [[0, np.int64(383)]],\n",
    " 'demo_17': [[0, np.int64(416)]],\n",
    " 'demo_18': [[0, np.int64(413)]],\n",
    " 'demo_19': [[0, np.int64(979)]],\n",
    " 'demo_2': [(np.int64(199), np.int64(730))],\n",
    " 'demo_20': [[0, np.int64(673)]],\n",
    " 'demo_21': [[0, np.int64(836)]],\n",
    " 'demo_22': [[0, np.int64(376)]],\n",
    " 'demo_23': [[0, np.int64(822)]],\n",
    " 'demo_24': [[0, np.int64(1143)]],\n",
    " 'demo_25': [[0, np.int64(617)]],\n",
    " 'demo_26': [[0, np.int64(662)]],\n",
    " 'demo_27': [[0, np.int64(1014)]],\n",
    " 'demo_28': [[0, np.int64(1120)]],\n",
    " 'demo_29': [[0, np.int64(447)]],\n",
    " 'demo_3': [],\n",
    " 'demo_30': [[0, np.int64(393)]],\n",
    " 'demo_31': [[0, np.int64(753)]],\n",
    " 'demo_32': [[0, np.int64(932)]],\n",
    " 'demo_33': [[0, np.int64(968)]],\n",
    " 'demo_34': [[0, np.int64(1134)]],\n",
    " 'demo_35': [[0, np.int64(735)]],\n",
    " 'demo_36': [[0, np.int64(594)]],\n",
    " 'demo_37': [[0, np.int64(329)]],\n",
    " 'demo_38': [[0, np.int64(388)]],\n",
    " 'demo_39': [[0, np.int64(882)]],\n",
    " 'demo_4': [[0, np.int64(816)]],\n",
    " 'demo_40': [[0, np.int64(451)]],\n",
    " 'demo_41': [[0, np.int64(435)]],\n",
    " 'demo_42': [[0, np.int64(1005)]],\n",
    " 'demo_43': [[0, np.int64(501)]],\n",
    " 'demo_44': [[0, np.int64(931)]],\n",
    " 'demo_45': [[0, np.int64(648)]],\n",
    " 'demo_46': [[0, np.int64(357)]],\n",
    " 'demo_47': [[0, np.int64(359)]],\n",
    " 'demo_48': [[0, np.int64(451)]],\n",
    " 'demo_49': [[0, np.int64(704)]],\n",
    " 'demo_5': [[0, np.int64(412)]],\n",
    " 'demo_50': [[0, np.int64(875)]],\n",
    " 'demo_51': [[0, np.int64(675)]],\n",
    " 'demo_52': [[0, np.int64(400)]],\n",
    " 'demo_53': [[0, np.int64(901)]],\n",
    " 'demo_54': [[0, np.int64(926)]],\n",
    " 'demo_55': [[0, np.int64(547)]],\n",
    " 'demo_56': [[0, np.int64(374)]],\n",
    " 'demo_57': [[0, np.int64(1000)]],\n",
    " 'demo_58': [[0, np.int64(775)]],\n",
    " 'demo_59': [[0, np.int64(1083)]],\n",
    " 'demo_6': [[0, np.int64(1102)]],\n",
    " 'demo_60': [[0, np.int64(417)]],\n",
    " 'demo_61': [[0, np.int64(397)]],\n",
    " 'demo_62': [[0, np.int64(347)]],\n",
    " 'demo_63': [[0, np.int64(378)]],\n",
    " 'demo_64': [[0, np.int64(832)]],\n",
    " 'demo_65': [[0, np.int64(412)]],\n",
    " 'demo_66': [[0, np.int64(752)]],\n",
    " 'demo_67': [[0, np.int64(373)]],\n",
    " 'demo_68': [[0, np.int64(707)]],\n",
    " 'demo_69': [[0, np.int64(970)]],\n",
    " 'demo_7': [[0, np.int64(861)]],\n",
    " 'demo_70': [[0, np.int64(352)]],\n",
    " 'demo_71': [[0, np.int64(544)]],\n",
    " 'demo_72': [[0, np.int64(404)]],\n",
    " 'demo_73': [[0, np.int64(425)]],\n",
    " 'demo_74': [[0, np.int64(1014)]],\n",
    " 'demo_75': [[0, np.int64(395)]],\n",
    " 'demo_76': [[0, np.int64(1227)]],\n",
    " 'demo_77': [[0, np.int64(365)]],\n",
    " 'demo_78': [[0, np.int64(384)]],\n",
    " 'demo_79': [[0, np.int64(367)]],\n",
    " 'demo_8': [[0, np.int64(598)]],\n",
    " 'demo_80': [[0, np.int64(332)]],\n",
    " 'demo_81': [[0, np.int64(831)]],\n",
    " 'demo_82': [[0, np.int64(661)]],\n",
    " 'demo_83': [[0, np.int64(648)]],\n",
    " 'demo_84': [[0, np.int64(411)]],\n",
    " 'demo_85': [[0, np.int64(380)]],\n",
    " 'demo_86': [[0, np.int64(354)]],\n",
    " 'demo_87': [[0, np.int64(984)]],\n",
    " 'demo_88': [[0, np.int64(427)]],\n",
    " 'demo_89': [[0, np.int64(531)]],\n",
    " 'demo_9': [[0, np.int64(373)]],\n",
    " 'demo_90': [[0, np.int64(394)]],\n",
    " 'demo_91': [[0, np.int64(408)]],\n",
    " 'demo_92': [[0, np.int64(387)]],\n",
    " 'demo_93': [[0, np.int64(544)]],\n",
    " 'demo_94': [[0, np.int64(366)]],\n",
    " 'demo_95': [[0, np.int64(743)]],\n",
    " 'demo_96': [[0, np.int64(392)]],\n",
    " 'demo_97': [[0, np.int64(552)]],\n",
    " 'demo_98': [[0, np.int64(384)]],\n",
    " 'demo_99': [[0, np.int64(354)]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_toremove_file = \"/home/ubuntu/dataset_mimicgen/segs/segs_index_s2i_square134_2_0ind_abs_s2i_g40b30_s2i.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using segs file:  /home/ubuntu/dataset_mimicgen/segs/segs_index_s2i_square134_2_0ind_abs_s2i_g40b30_s2i.json\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "print('using segs file: ', segments_toremove_file)\n",
    "with open(segments_toremove_file, 'r') as f:\n",
    "    data = json.load(f) \n",
    "data = data['data']\n",
    "segs_toremove=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[142, 314]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_name=\"demo_0\"\n",
    "\n",
    "segs_toremove[demo_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 332]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs_orginal[demo_name]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_segments(base, removals):\n",
    "    start, end = base\n",
    "    points = [(start, 'start')] + [(r[0], 'remove_start') for r in removals] + \\\n",
    "             [(r[1] + 1, 'remove_end') for r in removals] + [(end, 'end')]\n",
    "    points.sort()\n",
    "\n",
    "    active = 0\n",
    "    result = []\n",
    "    curr_start = None\n",
    "\n",
    "    for p, typ in points:\n",
    "        if typ == 'start' or typ == 'remove_end':\n",
    "            if active == 0:\n",
    "                curr_start = p\n",
    "            active += 1\n",
    "        elif typ == 'remove_start':\n",
    "            active -= 1\n",
    "            if active == 0 and curr_start is not None and curr_start < p:\n",
    "                result.append([curr_start, p - 1])\n",
    "        elif typ == 'end':\n",
    "            if active == 1 and curr_start is not None and curr_start < p:\n",
    "                result.append([curr_start, p])\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 9], [21, 29], [101, 332]]\n"
     ]
    }
   ],
   "source": [
    "original_seg = [0, 332]\n",
    "to_remove = [[10, 20], [30, 100]]\n",
    "\n",
    "\n",
    "result = subtract_segments(original_seg, to_remove)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo_name in segs_toremove.keys():\n",
    "    original_seg = segs_orginal[demo_name]\n",
    "    to_remove = segs_toremove[demo_name]\n",
    "    if len(to_remove) > 0:\n",
    "        result = subtract_segments(original_seg[0], to_remove)\n",
    "        segs_orginal[demo_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 167], [167, 514], [637, 652], [789, 898]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs_toremove['demo_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_map=segs_orginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=config.train.data\n",
    "obs_keys=shape_meta[\"all_obs_keys\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12341"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_kwargs = dict(\n",
    "    hdf5_path=data_path,\n",
    "    obs_keys=obs_keys,\n",
    "    dataset_keys=['actions'], \n",
    "    frame_stack=10,   \n",
    "    seq_length=1,\n",
    "    segments_map=segments_map,     # new parameter\n",
    "    pad_frame_stack=True,\n",
    "    pad_seq_length=True,\n",
    "    get_pad_mask=False, \n",
    "    hdf5_cache_mode= None, #'low_dim', #'all',\n",
    "    hdf5_use_swmr=True \n",
    ")\n",
    "\n",
    "trainset=MultiSegmentSequenceDataset(**ds_kwargs)\n",
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.experiment.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT will be forked to /home/ubuntu/mimicgen/mimicgen/../training_results/core/square/image/trained_models/square/20250430094304/logs/log.txt\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ubuntu/miniconda3/envs/equidiff/lib/python3.9/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: egl: failed to create dri2 screen\n",
      "libEGL warning: NEEDS EXTENSION: falling back to kms_swrast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: make sure gym is installed if you want to use the GymWrapper.\n",
      "Created environment with name NutAssemblySquare\n",
      "Action size is 7\n",
      "FrameStackWrapper(\n",
      "    num_frames=10\n",
      "    env=NutAssemblySquare\n",
      "    {\n",
      "        \"camera_depths\": true,\n",
      "        \"camera_heights\": 84,\n",
      "        \"camera_names\": [\n",
      "            \"agentview\",\n",
      "            \"robot0_eye_in_hand\"\n",
      "        ],\n",
      "        \"camera_widths\": 84,\n",
      "        \"controller_configs\": {\n",
      "            \"control_delta\": true,\n",
      "            \"damping_ratio\": 1,\n",
      "            \"damping_ratio_limits\": [\n",
      "                0,\n",
      "                10\n",
      "            ],\n",
      "            \"impedance_mode\": \"fixed\",\n",
      "            \"input_max\": 1,\n",
      "            \"input_min\": -1,\n",
      "            \"interpolation\": null,\n",
      "            \"kp\": 150,\n",
      "            \"kp_limits\": [\n",
      "                0,\n",
      "                300\n",
      "            ],\n",
      "            \"orientation_limits\": null,\n",
      "            \"output_max\": [\n",
      "                0.05,\n",
      "                0.05,\n",
      "                0.05,\n",
      "                0.5,\n",
      "                0.5,\n",
      "                0.5\n",
      "            ],\n",
      "            \"output_min\": [\n",
      "                -0.05,\n",
      "                -0.05,\n",
      "                -0.05,\n",
      "                -0.5,\n",
      "                -0.5,\n",
      "                -0.5\n",
      "            ],\n",
      "            \"position_limits\": null,\n",
      "            \"ramp_ratio\": 0.2,\n",
      "            \"type\": \"OSC_POSE\",\n",
      "            \"uncouple_pos_ori\": true\n",
      "        },\n",
      "        \"has_offscreen_renderer\": true,\n",
      "        \"has_renderer\": false,\n",
      "        \"ignore_done\": true,\n",
      "        \"render_gpu_device_id\": 0,\n",
      "        \"reward_shaping\": false,\n",
      "        \"robots\": \"Panda\",\n",
      "        \"use_camera_obs\": true,\n",
      "        \"use_object_obs\": true\n",
      "    }\n",
      ")\n",
      "\n",
      "/home/ubuntu/miniconda3/envs/equidiff/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/equidiff/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Created GPT_Backbone model with number of parameters: 18906112\n",
      "\n",
      "============= Training Dataset =============\n",
      "MultiSegmentSequenceDataset (\n",
      "\tpath=/home/ubuntu/dataset_mimicgen/square134_2_0ind_g40b30.hdf5\n",
      "\tobs_keys=('agentview_image', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_eye_in_hand_image', 'robot0_gripper_qpos')\n",
      "\tseq_length=1\n",
      "\tfilter_key=none\n",
      "\tframe_stack=10\n",
      "\tpad_seq_length=True\n",
      "\tpad_frame_stack=True\n",
      "\tgoal_mode=none\n",
      "\tcache_mode=none\n",
      "\tnum_demos=70\n",
      "\tnum_sequences=12341\n",
      ")\n",
      "\n",
      "**************************************************\n",
      "Warnings generated by robomimic have been duplicated here (from above) for convenience. Please check them carefully.\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No private macro file found!\n",
      "    It is recommended to use a private macro file\n",
      "    To setup, run: python /home/ubuntu/miniconda3/envs/equidiff/lib/python3.9/site-packages/robomimic/scripts/setup_macros.py\n",
      ")\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "100%|##########| 500/500 [00:39<00:00, 12.72it/s]\n",
      "Train Epoch 1\n",
      "{\n",
      "    \"Log_Likelihood\": 11.106713173627853,\n",
      "    \"Loss\": -11.106713173627853,\n",
      "    \"Optimizer/policy0_lr\": 0.00010000000000000003,\n",
      "    \"Policy_Grad_Norms\": 71844.16388305697,\n",
      "    \"Time_Data_Loading\": 0.03048915465672811,\n",
      "    \"Time_Epoch\": 0.6559552709261577,\n",
      "    \"Time_Log_Info\": 0.04059155782063802,\n",
      "    \"Time_Process_Batch\": 0.03986982901891072,\n",
      "    \"Time_Train_Batch\": 0.5428434411684672\n",
      "}\n",
      "\n",
      "Epoch 1 Memory Usage: 2190 MB\n",
      "\n",
      " 92%|#########1| 459/500 [00:36<00:03, 12.82it/s]"
     ]
    }
   ],
   "source": [
    "log_dir, ckpt_dir, video_dir = TrainUtils.get_exp_dir(config)\n",
    "\n",
    "if config.experiment.logging.terminal_output_to_txt:\n",
    "    # log stdout and stderr to a text file\n",
    "    logger = PrintLogger(os.path.join(log_dir, 'log.txt'))\n",
    "    sys.stdout = logger\n",
    "    sys.stderr = logger\n",
    "\n",
    " \n",
    "envs = OrderedDict()\n",
    "if config.experiment.rollout.enabled:\n",
    "    # create environments for validation runs\n",
    "    env_names = [env_meta[\"env_name\"]]\n",
    "\n",
    "    if config.experiment.additional_envs is not None:\n",
    "        for name in config.experiment.additional_envs:\n",
    "            env_names.append(name)\n",
    "\n",
    "    for env_name in env_names:\n",
    "        env = EnvUtils.create_env_from_metadata(\n",
    "            env_meta=env_meta,\n",
    "            env_name=env_name, \n",
    "            render=False, \n",
    "            render_offscreen=config.experiment.render_video,\n",
    "            use_image_obs=shape_meta[\"use_images\"], \n",
    "        )\n",
    "        env = EnvUtils.wrap_env_from_config(env, config=config) # apply environment warpper, if applicable\n",
    "        envs[env.name] = env\n",
    "        print(envs[env.name])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# setup for a new training run\n",
    "data_logger = DataLogger(\n",
    "    log_dir,\n",
    "    config,\n",
    "    log_tb=config.experiment.logging.log_tb,\n",
    "    log_wandb=config.experiment.logging.log_wandb,\n",
    ")\n",
    "model = algo_factory(\n",
    "    algo_name=config.algo_name,\n",
    "    config=config,\n",
    "    obs_key_shapes=shape_meta[\"all_shapes\"],\n",
    "    ac_dim=shape_meta[\"ac_dim\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# save the config as a json file\n",
    "with open(os.path.join(log_dir, '..', 'config.json'), 'w') as outfile:\n",
    "    json.dump(config, outfile, indent=4)\n",
    "\n",
    " \n",
    "# load training data\n",
    "# trainset, validset = TrainUtils.load_data_for_training(\n",
    "#     config, obs_keys=shape_meta[\"all_obs_keys\"])\n",
    "\n",
    "validset=None\n",
    "\n",
    "\n",
    "train_sampler = trainset.get_dataset_sampler() \n",
    "print(\"\\n============= Training Dataset =============\")\n",
    "print(trainset)\n",
    "print(\"\") \n",
    "\n",
    "# maybe retreve statistics for normalizing observations\n",
    "obs_normalization_stats = None\n",
    "if config.train.hdf5_normalize_obs:\n",
    "    obs_normalization_stats = trainset.get_obs_normalization_stats()\n",
    "\n",
    "# initialize data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=trainset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=config.train.batch_size,\n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=config.train.num_data_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "if config.experiment.validate:\n",
    "    # cap num workers for validation dataset at 1\n",
    "    num_workers = min(config.train.num_data_workers, 1)\n",
    "    valid_sampler = validset.get_dataset_sampler()\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=validset,\n",
    "        sampler=valid_sampler,\n",
    "        batch_size=config.train.batch_size,\n",
    "        shuffle=(valid_sampler is None),\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True\n",
    "    )\n",
    "else:\n",
    "    valid_loader = None\n",
    "\n",
    "# print all warnings before training begins\n",
    "print(\"*\" * 50)\n",
    "print(\"Warnings generated by robomimic have been duplicated here (from above) for convenience. Please check them carefully.\")\n",
    "flush_warnings()\n",
    "print(\"*\" * 50)\n",
    "print(\"\")\n",
    "\n",
    "# main training loop\n",
    "best_valid_loss = None\n",
    "best_return = {k: -np.inf for k in envs} if config.experiment.rollout.enabled else None\n",
    "best_success_rate = {k: -1. for k in envs} if config.experiment.rollout.enabled else None\n",
    "last_ckpt_time = time.time()\n",
    "\n",
    "# number of learning steps per epoch (defaults to a full dataset pass)\n",
    "train_num_steps = config.experiment.epoch_every_n_steps\n",
    "valid_num_steps = config.experiment.validation_epoch_every_n_steps\n",
    "\n",
    "for epoch in range(1, config.train.num_epochs + 1): # epoch numbers start at 1\n",
    "    step_log = TrainUtils.run_epoch(\n",
    "        model=model,\n",
    "        data_loader=train_loader,\n",
    "        epoch=epoch,\n",
    "        num_steps=train_num_steps,\n",
    "        obs_normalization_stats=obs_normalization_stats,\n",
    "    )\n",
    "    model.on_epoch_end(epoch)\n",
    "\n",
    "    # setup checkpoint path\n",
    "    epoch_ckpt_name = \"model_epoch_{}\".format(epoch)\n",
    "\n",
    "    # check for recurring checkpoint saving conditions\n",
    "    should_save_ckpt = False\n",
    "    if config.experiment.save.enabled:\n",
    "        time_check = (config.experiment.save.every_n_seconds is not None) and \\\n",
    "            (time.time() - last_ckpt_time > config.experiment.save.every_n_seconds)\n",
    "        epoch_check = (config.experiment.save.every_n_epochs is not None) and \\\n",
    "            (epoch > 0) and (epoch % config.experiment.save.every_n_epochs == 0)\n",
    "        epoch_list_check = (epoch in config.experiment.save.epochs)\n",
    "        should_save_ckpt = (time_check or epoch_check or epoch_list_check)\n",
    "    ckpt_reason = None\n",
    "    if should_save_ckpt:\n",
    "        last_ckpt_time = time.time()\n",
    "        ckpt_reason = \"time\"\n",
    "\n",
    "    print(\"Train Epoch {}\".format(epoch))\n",
    "    print(json.dumps(step_log, sort_keys=True, indent=4))\n",
    "    for k, v in step_log.items():\n",
    "        if k.startswith(\"Time_\"):\n",
    "            data_logger.record(\"Timing_Stats/Train_{}\".format(k[5:]), v, epoch)\n",
    "        else:\n",
    "            data_logger.record(\"Train/{}\".format(k), v, epoch)\n",
    "\n",
    "    # Evaluate the model on validation set\n",
    "    if config.experiment.validate:\n",
    "        with torch.no_grad():\n",
    "            step_log = TrainUtils.run_epoch(model=model, data_loader=valid_loader, epoch=epoch, validate=True, num_steps=valid_num_steps)\n",
    "        for k, v in step_log.items():\n",
    "            if k.startswith(\"Time_\"):\n",
    "                data_logger.record(\"Timing_Stats/Valid_{}\".format(k[5:]), v, epoch)\n",
    "            else:\n",
    "                data_logger.record(\"Valid/{}\".format(k), v, epoch)\n",
    "\n",
    "        print(\"Validation Epoch {}\".format(epoch))\n",
    "        print(json.dumps(step_log, sort_keys=True, indent=4))\n",
    "\n",
    "        # save checkpoint if achieve new best validation loss\n",
    "        valid_check = \"Loss\" in step_log\n",
    "        if valid_check and (best_valid_loss is None or (step_log[\"Loss\"] <= best_valid_loss)):\n",
    "            best_valid_loss = step_log[\"Loss\"]\n",
    "            if config.experiment.save.enabled and config.experiment.save.on_best_validation:\n",
    "                epoch_ckpt_name += \"_best_validation_{}\".format(best_valid_loss)\n",
    "                should_save_ckpt = True\n",
    "                ckpt_reason = \"valid\" if ckpt_reason is None else ckpt_reason\n",
    "\n",
    "    # Evaluate the model by by running rollouts\n",
    "\n",
    "    # do rollouts at fixed rate or if it's time to save a new ckpt\n",
    "    video_paths = None\n",
    "    rollout_check = (epoch % config.experiment.rollout.rate == 0) or (should_save_ckpt and ckpt_reason == \"time\")\n",
    "    if config.experiment.rollout.enabled and (epoch > config.experiment.rollout.warmstart) and rollout_check:\n",
    "\n",
    "        # wrap model as a RolloutPolicy to prepare for rollouts\n",
    "        rollout_model = RolloutPolicy(model, obs_normalization_stats=obs_normalization_stats)\n",
    "\n",
    "        num_episodes = config.experiment.rollout.n\n",
    "        all_rollout_logs, video_paths = TrainUtils.rollout_with_stats(\n",
    "            policy=rollout_model,\n",
    "            envs=envs,\n",
    "            horizon=config.experiment.rollout.horizon,\n",
    "            use_goals=config.use_goals,\n",
    "            num_episodes=num_episodes,\n",
    "            render=False,\n",
    "            video_dir=video_dir if config.experiment.render_video else None,\n",
    "            epoch=epoch,\n",
    "            video_skip=config.experiment.get(\"video_skip\", 5),\n",
    "            terminate_on_success=config.experiment.rollout.terminate_on_success,\n",
    "        )\n",
    "\n",
    "        # summarize results from rollouts to tensorboard and terminal\n",
    "        for env_name in all_rollout_logs:\n",
    "            rollout_logs = all_rollout_logs[env_name]\n",
    "            for k, v in rollout_logs.items():\n",
    "                if k.startswith(\"Time_\"):\n",
    "                    data_logger.record(\"Timing_Stats/Rollout_{}_{}\".format(env_name, k[5:]), v, epoch)\n",
    "                else:\n",
    "                    data_logger.record(\"Rollout/{}/{}\".format(k, env_name), v, epoch, log_stats=True)\n",
    "\n",
    "            print(\"\\nEpoch {} Rollouts took {}s (avg) with results:\".format(epoch, rollout_logs[\"time\"]))\n",
    "            print('Env: {}'.format(env_name))\n",
    "            print(json.dumps(rollout_logs, sort_keys=True, indent=4))\n",
    "\n",
    "        # checkpoint and video saving logic\n",
    "        updated_stats = TrainUtils.should_save_from_rollout_logs(\n",
    "            all_rollout_logs=all_rollout_logs,\n",
    "            best_return=best_return,\n",
    "            best_success_rate=best_success_rate,\n",
    "            epoch_ckpt_name=epoch_ckpt_name,\n",
    "            save_on_best_rollout_return=config.experiment.save.on_best_rollout_return,\n",
    "            save_on_best_rollout_success_rate=config.experiment.save.on_best_rollout_success_rate,\n",
    "        )\n",
    "        best_return = updated_stats[\"best_return\"]\n",
    "        best_success_rate = updated_stats[\"best_success_rate\"]\n",
    "        epoch_ckpt_name = updated_stats[\"epoch_ckpt_name\"]\n",
    "        should_save_ckpt = (config.experiment.save.enabled and updated_stats[\"should_save_ckpt\"]) or should_save_ckpt\n",
    "        if updated_stats[\"ckpt_reason\"] is not None:\n",
    "            ckpt_reason = updated_stats[\"ckpt_reason\"]\n",
    "\n",
    "    # Only keep saved videos if the ckpt should be saved (but not because of validation score)\n",
    "    should_save_video = (should_save_ckpt and (ckpt_reason != \"valid\")) or config.experiment.keep_all_videos\n",
    "    if video_paths is not None and not should_save_video:\n",
    "        for env_name in video_paths:\n",
    "            os.remove(video_paths[env_name])\n",
    "\n",
    "    # Save model checkpoints based on conditions (success rate, validation loss, etc)\n",
    "    if should_save_ckpt:\n",
    "        TrainUtils.save_model(\n",
    "            model=model,\n",
    "            config=config,\n",
    "            env_meta=env_meta,\n",
    "            shape_meta=shape_meta,\n",
    "            ckpt_path=os.path.join(ckpt_dir, epoch_ckpt_name + \".pth\"),\n",
    "            obs_normalization_stats=obs_normalization_stats,\n",
    "        )\n",
    "\n",
    "    # Finally, log memory usage in MB\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_usage = int(process.memory_info().rss / 1000000)\n",
    "    data_logger.record(\"System/RAM Usage (MB)\", mem_usage, epoch)\n",
    "    print(\"\\nEpoch {} Memory Usage: {} MB\\n\".format(epoch, mem_usage))\n",
    "\n",
    "# terminate logging\n",
    "data_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equidiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
